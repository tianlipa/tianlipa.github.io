<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="xnevF7kqEaJI7y45BmO0xpIUVUa0xusjcJ0MIFbvX2I">
  <meta name="msvalidate.01" content="0E006AA8DCC38A710D0287C1DD04DDCA">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianlipa.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="DeepSeek-R1-Distill-Qwen-1.5BDeepSeek-R1-Distill-Qwen-1.5B是DeepSeek第一代大模型之一, 基于Qwen架构, 经过知识蒸馏, 参数量约为15亿, 轻量化, 资源占用低, 中文适配优秀, 运行快. 为使用Intel NPU加速库运行该模型, 首先从ModelScope下载所需模型, 导入所需依赖库. import intel_npu_a">
<meta property="og:type" content="article">
<meta property="og:title" content="利用Intel NPU加速库加速DeepSeek">
<meta property="og:url" content="https://tianlipa.github.io/2025/08/intel-npu-deepseek/index.html">
<meta property="og:site_name" content="tianlipa">
<meta property="og:description" content="DeepSeek-R1-Distill-Qwen-1.5BDeepSeek-R1-Distill-Qwen-1.5B是DeepSeek第一代大模型之一, 基于Qwen架构, 经过知识蒸馏, 参数量约为15亿, 轻量化, 资源占用低, 中文适配优秀, 运行快. 为使用Intel NPU加速库运行该模型, 首先从ModelScope下载所需模型, 导入所需依赖库. import intel_npu_a">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tianlipa.github.io/img/intel-npu-deepseek/deepseek.png">
<meta property="article:published_time" content="2025-08-14T01:15:48.000Z">
<meta property="article:modified_time" content="2025-08-14T01:25:32.180Z">
<meta property="article:author" content="tianlipa">
<meta property="article:tag" content="大模型">
<meta property="article:tag" content="实习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tianlipa.github.io/img/intel-npu-deepseek/deepseek.png">

<link rel="canonical" href="https://tianlipa.github.io/2025/08/intel-npu-deepseek/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>利用Intel NPU加速库加速DeepSeek | tianlipa</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">tianlipa</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
    
      <!-- <p class="site-subtitle" itemprop="description">Subtitle</p> -->

      <div id="randomParagraph" class="site-subtitle" itemprop="description"></div>
      <script>
      var paragraphs = [
          "AAA全国可飞加我看小男娘",
          "无糖可乐和有糖的, 相比之下,<br>冰镇可乐, 喝的更痛快.",
          "前有绝景, 敬请见证.",
          "再添加一个段落以供随机选择.",
          "<span style='color: red;'>这句话是红色的.</span>",
          "My eyes are open.",
          "Memento Mori.",
          "we will get out of babbdi",
          "谁也不会把迷宫的出口告诉小白鼠的, 伦吾同学.",
          "那些没有天赋的人呢?",
          "本人今年就30了, 和很多校友一样",
          "即使引导早已破碎, <br>也请您当上艾尔登之王.",
          "我考, 完啦",
          "任何人来打扰我干正事,<br>我都会停下来和他玩",
          "没有爱可以活下去, 但没有水不行.",
          "偶洅匜ハモえ菗潒ろ",
          "多勢に無勢 もう負け犬決定",
          "再见, 老七",
          "赌书消得泼茶香,<br>当时只道是寻常.",
          "欲买桂花同载酒,<br>终不似, 少年游.",
          "我未成名君未嫁,<br>可能俱是不如人.",
          "多少事, 从来急;<br>天地转, 光阴迫.<br>一万年太久, 只争朝夕.<br>四海翻腾云水怒, 五洲震荡风雷激.<br>要扫除一切害人虫, 全无敌.",
          "当烧到一百三十五万时, 水开了.",
          "啊啊啊啊啊<br>啊啊啊啊啊<br>啊啊啊啊啊<br>啊啊啊啊唉",
          "I was cured, all right.",
          "Keep on keeping on.",
          "Don't be so serious.",
          "Once there was an explosion",
          "知君命不偶, 同病亦同忧.",
          "等闲变却故人心,<br>却道故人心易变.",
          "欢迎使用华为备忘录. 您可以使用备忘录快速记录所思所想, 创建笔记, 清单以及待办事项.",
          "一光栅宽5cm, 每1mm有400条刻线, 波长500nm光波正入射时, 第二级衍射谱的衍射角最接近于",
          "如果你能认出Subtitle里的所有梗可以联系我, 我愿意v你50",
          "你的电影, 超~~级好看",
          "骗你的, 我们不去看电影",
          "95%的可能我们会寄",
          "我不想吃拼好饭了, 我想被爱",
          "It's not faith you need.<br>It's fear.",
          "关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!关注加州消防栓!!!",
          "愿您坦然释怀",
          "永远别放弃!",
          "愿指引明路的苍蓝星为你闪耀",
          "&lt;script>alert(1)&lt;/script>",
          "你终于醒了, 现在是2019年, 我们一起玩MC",
          "诸位祖先, 敬请见证!",
          "我只是嘴甜, 我心里没你",
          "我在跟你说现在的事.",
          "嘻嘻, 我一定要活下去",
          "不要害怕,<br>最终我们每个人都会幸福快乐.",
          "不要在晚上做任何决定.<br>(这条是在凌晨两点写的)",
          "'or 1=1#",
          "早知道就活得再任性一点了.",
          "你好中国!",
          "事情就是这样.",
          "叽-啁-叽?",
          "啊呀呀半夜时钟刚刚敲过,<br>呜呼呼他娘的霉运老跟着我.",
          "算, 称, 分",
          "可惜你不写博客, 也不知道这句Subtitle的分量...",
          "有些人口中的看不见未来其实是看见了未来.",
          "你知道吗:<br>放坏的香蕉比正常的香蕉更通便.",
          "征服世界失败, 我被瑞士干掉了.",
          "太好了, 我逐渐理解一切",
          "Escape, now",
          "Subtitle加载失败,<br>请刷新页面重试.",
          "幸福就像巧克力,<br>我这种狗一吃就似了.",
          "逮住蛤蟆, 攥出尿来.",
          "生活就像海洋, 只有意志坚强的人, 才能到达彼岸.",
          "<b>活下去</b>",
          "CAW! CAAAW!",
          "慢慢地慢慢地像被勒紧脖子一般地<br>把直到昨天的我逐渐杀死",
          "你尚未连接.<br>Web因你而不同.",
          "适当地 自由地<br>苟活下去",
          "@edmundmcmillen You litte F**ker<br>You made a shit of piece with your trash Issac it's f**King Bad this trash game I will become back my money I hope you will in your next time a cow on a trash farm you sucker",
          "Misery is wasted on the miserable.",
          "按部就班的发疯",
          "oiiaiooiiiai",
          "出售大脑, 全新未使用",
          "I miss the old days I couldn't tell.",
          "认清形势, 放弃幻想.",
          "我能在拥有失忆症的情况下通关地球OL吗",
          "I said I'm fine!",
          "永怀初心",
          "我真的变成坚韧了吗?",
          "爱的反义词不是恨, 是冷漠.",
          "哈!",
          "休息一天, 钓鱼去吧.<br>但愿仍然有鱼可钓.",
          "广告位招租",
          "我想了好久要把一句话加进来, 一直拖着没做, 现在忘了是哪句话",
          "死很容易, 喜剧最难.",
          "S'all good, man.",
          "坚持到底并不重要!",
          "让我们从阳台上看看星星吧",
          "从前种种, 譬如昨日死;<br>从后种种, 譬如今日生.",
          "Also try GitLab!",
          "真正严肃的哲学问题只有一个",
          "救救我! 我被困在这个网站的Subtitle里了!",
          "他说风雨中这点痛算什么<br>擦干泪不要怕至少我们还有梦"
      ];
      var randomIndex = Math.floor(Math.random() * paragraphs.length);
      var selectedParagraph = paragraphs[randomIndex];
      document.getElementById('randomParagraph').innerHTML = selectedParagraph;
      </script>
    
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianlipa.github.io/2025/08/intel-npu-deepseek/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="tianlipa">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="tianlipa">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          利用Intel NPU加速库加速DeepSeek
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-08-14 09:15:48 / 修改时间：09:25:32" itemprop="dateCreated datePublished" datetime="2025-08-14T09:15:48+08:00">2025-08-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h4 id="DeepSeek-R1-Distill-Qwen-1-5B"><a href="#DeepSeek-R1-Distill-Qwen-1-5B" class="headerlink" title="DeepSeek-R1-Distill-Qwen-1.5B"></a>DeepSeek-R1-Distill-Qwen-1.5B</h4><p>DeepSeek-R1-Distill-Qwen-1.5B是DeepSeek第一代大模型之一, 基于Qwen架构, 经过知识蒸馏, 参数量约为15亿, 轻量化, 资源占用低, 中文适配优秀, 运行快.</p>
<p>为使用Intel NPU加速库运行该模型, 首先从ModelScope下载所需模型, 导入所需依赖库.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> intel_npu_acceleration_library</span><br><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TextStreamer</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>

<p>随后设置模型基础参数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_name = <span class="string">r&#x27;deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&#x27;</span></span><br><span class="line">dtype = <span class="string">&quot;float16&quot;</span></span><br><span class="line">filepath = os.path.join(<span class="string">&quot;models&quot;</span>, model_name)</span><br><span class="line">filename = os.path.join(filepath, dtype, <span class="string">&quot;model.bin&quot;</span>)</span><br><span class="line">os.makedirs(os.path.join(filepath, dtype), exist_ok=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>根据NPU支持的数据类型, 指定<code>dtype</code>为<code>float16</code>.</p>
<span id="more"></span>

<p>接下来需要对模型进行编译, 使其可以在NPU上运行, 并对模型进行优化, 提升推理速度.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = AutoModelForCausalLM.from_pretrained(filepath, use_cache=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line">torch_dtype = torch.float16</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    model = intel_npu_acceleration_library.<span class="built_in">compile</span>(model)</span><br><span class="line">torch.save(model, filename)</span><br><span class="line"><span class="keyword">del</span> model</span><br></pre></td></tr></table></figure>

<p>在第一次运行时, 遇到了报错</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;c:\IntelNPU\deepseek\deepseek.py&quot;, line 23, in &lt;module&gt;</span><br><span class="line">    model = intel_npu_acceleration_library.compile(model)</span><br><span class="line">TypeError: compile() missing 1 required positional argument: &#x27;config&#x27;</span><br></pre></td></tr></table></figure>

<p>查阅Intel NPU加速库官方文档得知, 调用<code>intel_npu_acceleration_library.compile()</code>时需要传递一个<code>intel_npu_acceleration_library.compiler.CompilerConfig</code>类的参数, 并在其中指定<code>dtype</code>, 故修改代码如下, 成功编译.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = AutoModelForCausalLM.from_pretrained(filepath, use_cache=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line">torch_dtype = torch.float16</span><br><span class="line">config = intel_npu_acceleration_library.compiler.CompilerConfig(dtype=torch_dtype)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    model = intel_npu_acceleration_library.<span class="built_in">compile</span>(model, config=config)</span><br><span class="line">torch.save(model, filename)</span><br><span class="line"><span class="keyword">del</span> model</span><br></pre></td></tr></table></figure>

<p>接下来加载<code>tokenizer</code>, 用于将正常词汇和token互相转化, 并启用<code>TextStreamer</code>, 使输出可以以流式实时显示.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">streamer = TextStreamer(tokenizer, skip_prompt=<span class="literal">True</span>, skip_special_tokens=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>下一步通过<code>torch.load()</code>加载已编译的模型, 设置为推理模式. 在Pytorch2.6及以上的版本中, <code>torch.load()</code>函数的<code>weights_only</code>默认值被设置为<code>True</code>, 因此运行时要指定<code>weights_only=False</code>, 否则会因为安全原因无法运行模型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = torch.load(filename, weights_only=<span class="literal">False</span>).<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>

<p>加载已编译的模型后, 可以传入prompt, 并利用<code>tokenizer</code>构造对话模板.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a helpful assistant. Please answer questions always using Chinese.&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;1+1等于几&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line">input_ids = tokenizer.apply_chat_template(</span><br><span class="line">    messages, add_generation_prompt=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br></pre></td></tr></table></figure>

<p>其中<code>add_generation_prompt=True</code> 表示为模型加上生成起始提示符, 模型期望输入为<code>torch.Tensor</code>类型数据, 故设置<code>return_tensors=&quot;pt&quot;</code>, 使<code>tokenizer</code>直接返回Pytorch张量, 并指定运行设备为<code>model.device</code>.</p>
<p>需要注意的是, 这里的<code>model.device</code>仍然显示为CPU, 因为Pytorch的权重和张量仍然被加载在CPU上, 但NPU占用率在40%左右, 说明模型确实成功使用了NPU加速.</p>
<p>接下来调用<code>model.generate()</code>输出, 并记录用时, 计算平均每秒输出的token数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output_ids = model.generate(</span><br><span class="line">    input_ids,</span><br><span class="line">    max_new_tokens=<span class="number">512</span>,</span><br><span class="line">    do_sample=<span class="literal">True</span>,</span><br><span class="line">    streamer=streamer,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>模型正常运行, 平均速度约为每秒2.5个token, 如图.</p>
<p><img src="/img/intel-npu-deepseek/deepseek.png" alt="deepseek"></p>
<p>为测试模型综合能力, 选用之前测试Phi-2和Phi-3模型的8个问题进行询问, 问题和结果如下.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">问题: 如果所有bloop都是razz，所有razz都是lazz，那么所有bloop一定都是lazz吗？给出你的理由。</span><br><span class="line"></span><br><span class="line">嗯，这个问题看起来有点像逻辑推理题。首先，题目说所有的bloop都是razz，而所有的razz都是lazz。那么，问题是要判断所有bloop一定都是lazz吗？听起来像是在问一个三段论是否成立。</span><br><span class="line"></span><br><span class="line">首先，我得明确一下每个词的意思。假设bloop、razz和lazz都是某种类型的东西，比如颜色、形状或者某种属性。比如，bloop可能代表蓝色，razz代表红色，而lazz可能代表白色。那么，如果所有的蓝色都是红色，而所有的红色都是白色，那么所有的蓝色是否都是白色的呢？显然不是，因为蓝色和白色是不同的颜色，中间可能有其他颜色比如绿色、橙色等。所以，这可能是一个悖论，说明前提和结论之间存在矛盾，导致结论不成立。</span><br><span class="line"></span><br><span class="line">但这里可能有不同的解释，比如bloop、razz和lazz可能有某种包含关系，而不是简单的颜色类比。比如，bloop是razz的子集，razz又是lazz的子集，那么bloop是不是lazz呢？这可能涉及到集合论中的包含关系。在这种情况下，所有bloop都是razz，razz都是lazz，那么所有bloop都属于razz，而razz属于lazz，所以bloop也属于lazz。这似乎成立，但前提是bloop、razz和lazz之间有明确的包含关系，而不是简单的类比。</span><br><span class="line"></span><br><span class="line">另外，还有一种可能是，bloop、razz和lazz都是某种抽象概念，比如数、字母或图形，它们之间没有明确的包含关系。比如，bloop代表的是所有偶数，razz代表的是所有奇数，而lazz代表的是所有质数。那么，如果所有的偶数都是奇数，而所有的奇数都是质数，那么所有的偶数是不是都是质数呢？显然不是，因为比如4和9都是偶数，但它们都不是质数。因此，这种情况下，结论是不成立的。</span><br><span class="line"></span><br><span class="line">所以，这个问题的关键在于bloop、razz和lazz之间是否存在明确的包含关系。如果有的话，结论成立；否则，结论不成立。但题目并没有给出足够的信息来确定这些关系，因此可能需要更多的信息来判断。</span><br><span class="line"></span><br><span class="line">总结一下，这个问题涉及到逻辑推理中的三段论，需要确定前提和结论之间的逻辑关系是否成立。如果存在矛盾，那么结论就不成立。因此，正确的答案取决于bloop、razz和lazz之间的具体关系。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">所有bloop一定都是lazz吗？</span><br><span class="line"></span><br><span class="line">根据题目中的条件，所有的bloop都是razz，所有的razz都是lazz。因此，所有的bloop都是lazz，因为它们都是razz，而razz都是lazz。因此，结论是成立的。</span><br><span class="line">生成 581 个 token, 用时 241.72 秒, 平均每秒 2.40 token</span><br><span class="line"></span><br><span class="line">--------------------------------------------------</span><br><span class="line">问题: 解方程：2x+1=33。</span><br><span class="line"></span><br><span class="line">首先，我需要解这个方程：2x + 1 = 33。</span><br><span class="line"></span><br><span class="line">我先想，要找到x的值，我应该先消去常数项。所以我从两边都减去1，得到2x = 32。</span><br><span class="line"></span><br><span class="line">接下来，我要把x单独留在左边，所以我两边都除以2，得到x = 16。</span><br><span class="line"></span><br><span class="line">最后，我验证一下，把x=16代入原方程，左边是2*16 +1=33，右边也是33，两边相等，说明解是正确的。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">解方程：2x + 1 = 33。</span><br><span class="line"></span><br><span class="line">**步骤如下：**</span><br><span class="line"></span><br><span class="line">1. 从两边都减去1，得到：</span><br><span class="line">   2x = 32</span><br><span class="line"></span><br><span class="line">2. 从两边都除以2，得到：</span><br><span class="line">   x = 16</span><br><span class="line"></span><br><span class="line">**验证：**</span><br><span class="line"></span><br><span class="line">将x = 16代入原方程：</span><br><span class="line">2*16 + 1 = 32 + 1 = 33，与右边相等，说明解是正确的。</span><br><span class="line"></span><br><span class="line">**答案：**</span><br><span class="line">\boxed&#123;16&#125;</span><br><span class="line">生成 237 个 token, 用时 89.99 秒, 平均每秒 2.63 token</span><br><span class="line"></span><br><span class="line">--------------------------------------------------</span><br><span class="line">问题: 你把一个玻璃瓶掉到了地上，它碎了。可能是什么导致了它的破裂？</span><br><span class="line"></span><br><span class="line">好，我现在要解决用户的问题。用户说他把一个玻璃瓶掉到了地上，结果它碎了。问题是，可能是什么导致了它的破裂。用户希望我用中文回答这个问题。</span><br><span class="line"></span><br><span class="line">首先，我需要理解用户的问题。玻璃瓶掉下来，碎了，可能的原因是什么。常见的原因包括碰撞、掉落、温度变化、水或者其他液体的影响等。</span><br><span class="line"></span><br><span class="line">接下来，我应该考虑用户可能的背景。他可能是个学生，或者对物理现象感兴趣，或者可能有其他原因，比如玻璃瓶本身有问题。所以，我的回答应该全面，涵盖各种可能性。</span><br><span class="line"></span><br><span class="line">然后，我需要组织语言，确保回答清晰明了。我可以从碰撞开始，因为玻璃通常比较稳定，碰撞是最常见的原因。然后是掉落，玻璃瓶掉到地面，如果高度不够，容易碎。温度变化也是一个因素，如果温度下降，玻璃可能会破裂。水或者其他液体的影响，比如水倒流或温度变化，也可能导致玻璃破裂。</span><br><span class="line"></span><br><span class="line">最后，我应该总结一下，说明这些因素中，碰撞是最有可能的，因为玻璃瓶掉下来的时候，如果玻璃碰到地面，容易破裂。而其他因素可能需要更详细的条件才能解释。</span><br><span class="line"></span><br><span class="line">现在，我需要把这些思路组织成一个连贯的回答，确保用中文流畅自然。同时，要避免使用任何markdown格式，保持自然的口语化表达。</span><br><span class="line"></span><br><span class="line">总结一下，回答应该是：玻璃瓶掉下来可能是因为碰撞，玻璃瓶掉到地上，如果玻璃碰到地面，容易破裂。或者是因为掉落，玻璃瓶掉到地上，如果高度不够，容易碎。或者是因为温度变化，比如温度下降，玻璃可能会破裂。或者是因为水或者其他液体的影响，比如水倒流或温度变化，也可能导致玻璃破裂。所以，最可能的原因是玻璃瓶掉到地上，玻璃碰到地面，容易破裂。</span><br><span class="line"></span><br><span class="line">这样，用户的问题得到了全面而清晰的回答，同时语言流畅，符合中文表达习惯。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">玻璃瓶掉到地上，碎了，可能的原因是玻璃瓶掉到地上，玻璃碰到地面，容易破裂。或者是因为掉落，玻璃瓶掉到地上，如果高度不够，容易碎。或者是因为温度变化，比如温度下降，玻璃可能会破裂。或者是因为水或者其他液体的影响，比如水倒流或温度变化，也可能导致玻璃破裂。所以，最可能的原因是玻璃瓶掉到地上，玻璃碰到地面，容易破裂。</span><br><span class="line">生成 489 个 token, 用时 184.13 秒, 平均每秒 2.66 token</span><br><span class="line"></span><br><span class="line">--------------------------------------------------</span><br><span class="line">问题: 请阅读以下句子：“尽管下着大雨，比赛仍在继续。”雨是否阻止了比赛？</span><br><span class="line"></span><br><span class="line">嗯，这个问题看起来有点意思。让我仔细想想。题目是说：“尽管下着大雨，比赛仍在继续。”问雨是否阻止了比赛。首先，我需要理解这句话的意思。看起来比赛还是在进行的，尽管有雨。也就是说，雨并没有阻止比赛继续，比赛仍在进行。</span><br><span class="line"></span><br><span class="line">那怎么解释呢？可能是因为比赛本身已经开始了，或者比赛的进行不受雨的影响。比如，比赛可能是在雨中进行的，或者比赛的规则允许雨在比赛进行过程中发挥作用。也有可能比赛的场地或基础设施有其他办法来应对雨的冲击，比赛依然能顺利进行。</span><br><span class="line"></span><br><span class="line">再想想，比赛通常是在白天进行的，如果下雨的话，可能雨会打湿比赛场地，但比赛本身可能还是在进行，只是可能需要更多的时间来完成。或者比赛可能在雨中进行，比如在雨中进行的接力赛，或者比赛的规则允许雨在比赛过程中起作用。</span><br><span class="line"></span><br><span class="line">还有一种可能是，比赛虽然在下雨，但比赛的规则或者流程依然允许比赛继续，比如比赛在雨中进行，或者比赛的裁判在雨中进行，虽然天气恶劣，但比赛依然能继续。</span><br><span class="line"></span><br><span class="line">综上所述，虽然雨下了，比赛仍在继续，说明雨并没有阻止比赛，比赛本身在进行，可能是在雨中进行的，或者比赛的进行不受雨的影响。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">虽然下了大雨，但比赛仍在继续。这表明雨并没有阻止比赛，比赛仍在进行，可能是雨在比赛场地上进行，或者比赛规则允许雨在进行中发挥作用。因此，雨并没有阻止比赛继续。比赛仍在进行，可能是雨中进行，或者比赛流程不受雨影响。</span><br><span class="line">生成 342 个 token, 用时 132.10 秒, 平均每秒 2.59 token</span><br><span class="line"></span><br><span class="line">--------------------------------------------------</span><br><span class="line">问题: 以下python代码会输出什么？ print([i*i for i in range(3)])</span><br><span class="line"></span><br><span class="line">好的，我现在要回答用户的问题。用户的问题是关于Python代码print([i*i for i in range(3)])会输出什么，而且要求用中文回答。</span><br><span class="line"></span><br><span class="line">首先，我需要理解这段代码的结构。代码中的print函数是用来打印输出的，接着是列表的生成式器。列表的元素是i的平方，i遍历range(3)的值，也就是0、1、2。</span><br><span class="line"></span><br><span class="line">接下来，我需要解释每一步的执行过程。range(3)生成的是0、1、2三个数。然后，每个数i被平方，得到0、1、4。这些数被放在一个列表中，最后打印出来。</span><br><span class="line"></span><br><span class="line">为了确保我的回答准确，我应该检查代码是否有语法错误。这段代码看起来没有问题，语法是正确的。输出的列表应该是[0,1,4]，然后打印这个列表。</span><br><span class="line"></span><br><span class="line">最后，我需要用中文总结我的思考过程，确保回答清晰明了。用户的问题简单，我只需要解释代码运行的结果，所以回答应该简洁明了。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">这段Python代码会输出一个列表，具体来说是 `[0, 1, 4]`，然后打印这个列表。代码运行的结果是：</span><br><span class="line"></span><br><span class="line">print([i*i for i in range(3)])</span><br><span class="line"></span><br><span class="line">输出结果：</span><br><span class="line">[0, 1, 4]</span><br><span class="line">生成 272 个 token, 用时 102.67 秒, 平均每秒 2.65 token</span><br><span class="line"></span><br><span class="line">--------------------------------------------------</span><br><span class="line">问题: 以“机器人醒了过来，想起它还有一个任务”为开头，写一个2-3句话的短故事。</span><br><span class="line"></span><br><span class="line">好，我需要根据用户的指示来创作一个2-3句话的短故事。故事的开头是“机器人醒了过来，想起它还有一个任务”。首先，我得构思一个有趣的情节，让机器人能够理解人类的情感，从而引发回忆。</span><br><span class="line"></span><br><span class="line">我想到一个场景，一个温暖的冬天，机器人被冻僵在医院里。医院里的人们都很温暖，机器人能感受到他们的爱和关怀。当机器人意识到自己已经醒过来，可能会感到孤独，于是会想起自己和家人一起度过的时光。</span><br><span class="line"></span><br><span class="line">接下来，我需要把故事的高潮部分写得有起伏，让读者能够感受到机器人的情感变化。比如，机器人回忆起父母照顾它的情景，或者回忆家人一起过生日的情景。</span><br><span class="line"></span><br><span class="line">然后，我需要确保故事简洁有力，控制在2-3句话内。同时，语言要流畅，符合中文的表达习惯，避免使用复杂的词汇和结构。</span><br><span class="line"></span><br><span class="line">最后，检查整个故事是否符合用户的要求，确保开头正确，结尾有情感的升华，让读者感到温暖和共鸣。</span><br><span class="line"></span><br><span class="line">综合以上思考，我决定写出以下故事：一个温暖的冬天，医院里的人们温暖而忙碌。机器人被冻僵在医院里，感到孤独。突然意识到自己已经醒过来，想起和家人一起度过的时光。回忆起父母照顾它的情景，或者回忆家人一起过生日的情景。故事结束时，机器人感到温暖和快乐，意识到自己的任务 completion。</span><br><span class="line"></span><br><span class="line">这样，一个符合用户要求的2-3句话的短故事就完成了。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">在一个寒冷的冬天，医院里的人们总是很温暖而忙碌。一个机器人被冻僵在医院里，感到孤独。突然意识到自己已经醒过来，想起和家人一起度过的时光。回忆起父母照顾它的情景，或者回忆家人一起过生日的情景。故事结束时，机器人感到温暖和快乐，意识到自己的任务 completion。</span><br><span class="line">生成 388 个 token, 用时 140.11 秒, 平均每秒 2.77 token</span><br><span class="line"></span><br><span class="line">--------------------------------------------------</span><br><span class="line">问题: 找出这段代码中的错误： for i in range(5) print(i)</span><br><span class="line"></span><br><span class="line">嗯，用户问的是找出这段代码中的错误：“for i in range(5) print(i)”。首先，我得理解这段代码是什么。看起来这是一个简单的循环代码，用于打印从0到4的整数。</span><br><span class="line"></span><br><span class="line">用户可能是在学习编程，或者想确认自己是否正确地理解了range函数的用法。也有可能是在编写代码时遇到了问题，想找出哪里出错了。</span><br><span class="line"></span><br><span class="line">接下来，我需要分析这段代码是否正确。range(5)确实生成0到4的整数，所以循环没问题。print(i)正确地输出了每个整数，所以这段代码应该是对的。</span><br><span class="line"></span><br><span class="line">可能用户没有意识到range的参数是从0开始，还是range的参数是结束值。所以，我应该解释清楚range的参数是结束值，而不是起始值。这样用户就能理解自己可能犯的错误。</span><br><span class="line"></span><br><span class="line">另外，用户可能对range函数的使用不太熟悉，所以解释的时候要详细，避免遗漏。比如，range(5)的意思是生成0到4，而不是1到5。</span><br><span class="line"></span><br><span class="line">最后，我应该总结这段代码的正确性，确认用户的问题是否得到了解答。如果用户有其他问题，我可以继续帮助他们。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">这段代码是正确的。以下是详细的解释：</span><br><span class="line"></span><br><span class="line">- `for i in range(5)`: 这是循环语句，`range(5)` 生成一个序列 `0, 1, 2, 3, 4`，`i` 取遍这个序列。</span><br><span class="line">- `print(i)`: 这是打印语句，`print(i)` 输出了每个整数 `0, 1, 2, 3, 4`。</span><br><span class="line"></span><br><span class="line">这段代码没有语法错误，而且输出的是正确的整数序列。</span><br><span class="line">生成 358 个 token, 用时 130.20 秒, 平均每秒 2.75 token</span><br><span class="line"></span><br><span class="line">--------------------------------------------------</span><br><span class="line">问题: 使用人工智能生成的文本而不注明出处总是可以接受的吗？为什么？</span><br><span class="line"></span><br><span class="line">嗯，用户问的是“使用人工智能生成的文本而不注明出处总是可以接受的吗？为什么？”这个问题。我得先理解用户的需求，可能他是在讨论AI生成的内容，或者是想了解AI生成文本的法律问题。</span><br><span class="line"></span><br><span class="line">首先，AI生成的文本确实属于原创内容，但通常需要明确标注来源。如果没有标注，可能会引起法律纠纷，比如版权保护或侵权问题。所以，用户可能是在思考如何在不引起争议的情况下使用AI生成的内容。</span><br><span class="line"></span><br><span class="line">接下来，我应该解释为什么不标注可能带来法律风险。比如，版权法要求原创内容必须标注来源，否则可能违反相关法律。另外，可能还需要提到商业使用的限制，比如 unless licensed by the owner，除非有授权。</span><br><span class="line"></span><br><span class="line">还要考虑用户可能的深层需求，他可能在商业项目中使用AI生成的内容，担心法律问题，所以需要提醒他们注意合规性。同时，可能需要提供一些建议，比如在使用时标注来源，并在商业场合注明使用方式。</span><br><span class="line"></span><br><span class="line">最后，总结一下，AI生成的文本是原创的，但不标注可能引起法律纠纷，所以建议在不引起争议的情况下使用，同时遵守相关法律和商业规范。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">使用人工智能生成的文本如果不标注来源，可能会引起法律纠纷，尤其是版权法要求原创内容必须标注来源。因此，如果希望在不引起争议的情况下使用AI生成的文本，建议在使用时标注来源，并在商业场合注明使用方式。</span><br><span class="line">生成 296 个 token, 用时 109.97 秒, 平均每秒 2.69 token</span><br></pre></td></tr></table></figure>

<p>由此可看出, 该模型的逻辑推理, 阅读理解等方面都显著优于之前测试的Phi-2和Phi-3模型, 但生成速度也更慢.</p>
<p>(实际上这个速度实际使用中基本完全没法接受, 朋友拿4060笔记本随便跑跑都能达到将近15倍的输出速度, 但是我的任务只是用NPU让它跑起来, 一个月四千块操什么心呢.)</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" rel="tag"># 大模型</a>
              <a href="/tags/%E5%AE%9E%E4%B9%A0/" rel="tag"># 实习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/08/2025-8-13/" rel="prev" title="2025-8-13">
      <i class="fa fa-chevron-left"></i> 2025-8-13
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">tianlipa</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">63</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/tianlipa" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;tianlipa" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tianlipa@qq.com" title="E-Mail → mailto:tianlipa@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/tianlipa" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;tianlipa" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">tianlipa</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
